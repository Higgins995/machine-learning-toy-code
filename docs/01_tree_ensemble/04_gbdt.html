
<!DOCTYPE html>

<html lang="zh">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Part D: 梯度提升树（上） &#8212; Datawhale</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="Part E: 梯度提升树（下）" href="05_xgb_lgb.html" />
    <link rel="prev" title="Part C: 自适应提升法" href="03_ada.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Datawhale</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index.html">
   树模型与集成学习
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="01_tree.html">
     Part A: 决策树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02_ensemble.html">
     Part B: 集成模式
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03_ada.html">
     Part C: 自适应提升法
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Part D: 梯度提升树（上）
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05_xgb_lgb.html">
     Part E: 梯度提升树（下）
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02_em_sampling/index.html">
   EM算法与抽样理论
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02_em_sampling/01_em.html">
     EM算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02_em_sampling/02_sample.html">
     抽样理论
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/01_tree_ensemble/04_gbdt.md.txt"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gbdt">
   1. 用于回归的GBDT
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   2. 用于分类的GBDT
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   3. GBDT中的并行策略
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   代码实践
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   算法实现
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   知识回顾
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="part-d">
<h1>Part D: 梯度提升树（上）<a class="headerlink" href="#part-d" title="永久链接至标题">¶</a></h1>
<div class="section" id="gbdt">
<h2>1. 用于回归的GBDT<a class="headerlink" href="#gbdt" title="永久链接至标题">¶</a></h2>
<p>设数据集为<span class="math notranslate nohighlight">\(D=\{(X_1,y_1),...,(X_N,y_N)\}\)</span>，模型的损失函数为<span class="math notranslate nohighlight">\(L(y,\hat{y})\)</span>，现希望利用多棵回归决策树来进行模型集成：设第<span class="math notranslate nohighlight">\(m\)</span>轮时，已知前<span class="math notranslate nohighlight">\(m-1\)</span>轮中对第<span class="math notranslate nohighlight">\(i\)</span>个样本的集成输出为<span class="math notranslate nohighlight">\(F_{m-1}(X_i)\)</span>，则本轮的集成输出<span class="math notranslate nohighlight">\(\hat{y}_i\)</span>为</p>
<div class="math notranslate nohighlight">
\[
F_{m}(X_i)=F_{m-1}(X_i)+h_m(X_i)
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(h_m\)</span>是使得当前轮损失<span class="math notranslate nohighlight">\(\sum_{i=1}^N L(y_i,\hat{y}_i)\)</span>达到最小的决策树模型。</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>【练习】对于均方损失函数和绝对值损失函数，请分别求出模型的初始预测<span class="math notranslate nohighlight">\(F_{0}\)</span>。</p>
</div>
<p>特别地，当<span class="math notranslate nohighlight">\(m=0\)</span>时，<span class="math notranslate nohighlight">\(F_{0}(X_i)=\arg\min_{\hat{y}} \sum_{i=1}^N L(y_i,\hat{y})\)</span>。</p>
<p>记第<span class="math notranslate nohighlight">\(m\)</span>轮的损失函数为</p>
<div class="math notranslate nohighlight">
\[
G(h_m) = \sum_{i=1}^NL(y_i, F_{m-1}(X_i)+h_m(X_i))
\]</div>
<p>令上述损失最小化不同于一般的参数优化问题，我们需要优化的并不是某一组参数，而是要在所有决策树模型组成的函数空间中，找到一个<span class="math notranslate nohighlight">\(h^*\)</span>使得<span class="math notranslate nohighlight">\(G(h^*)\)</span>最小。因此我们不妨这样思考：学习一个决策树模型等价于对数据集<span class="math notranslate nohighlight">\(\tilde{{D}}=\{(X_1,h^*(X_1)),...,(X_N,h^*(X_N))\}\)</span>进行拟合，设<span class="math notranslate nohighlight">\(w_i=h^*(X_I)\)</span>，<span class="math notranslate nohighlight">\(\textbf{w}=[w_1,...,w_N]\)</span>，此时的损失函数可改记为</p>
<div class="math notranslate nohighlight">
\[
G(\textbf{w})=\sum_{i=1}^NL(y_i, F_{m-1}(X_i)+w_i)
\]</div>
<p>由于只要我们获得最优的<span class="math notranslate nohighlight">\(\textbf{w}\)</span>，就能拟合出第<span class="math notranslate nohighlight">\(m\)</span>轮相应的回归树，此时一个函数空间的优化问题已经被转换为了参数空间的优化问题，即对于样本<span class="math notranslate nohighlight">\(i\)</span>而言，最优参数为</p>
<div class="math notranslate nohighlight">
\[
w_i=\arg\min_{w}L(y_i,F_{m-1}(X_i)+w)
\]</div>
<p>对于可微的损失函数<span class="math notranslate nohighlight">\(L\)</span>，由于当<span class="math notranslate nohighlight">\(\textbf{w}=\textbf{0}\)</span>时的损失就是第<span class="math notranslate nohighlight">\(m-1\)</span>轮预测产生的损失，因此我们只需要在<span class="math notranslate nohighlight">\(w_i=0\)</span>处进行一步梯度下降（若能保证合适的学习率大小）就能够获得使损失更小的<span class="math notranslate nohighlight">\(w^*_i\)</span>，而这个值正是我们决策树需要拟合的<span class="math notranslate nohighlight">\(h^*(X_i)\)</span>。
以损失函数<span class="math notranslate nohighlight">\(L(y,\hat{y})=\sqrt{\vert y-\hat{y}\vert}\)</span>为例，记残差为</p>
<div class="math notranslate nohighlight">
\[
r_i = y_i-F_{m-1}(X_i)
\]</div>
<p>则实际损失为</p>
<div class="math notranslate nohighlight">
\[
L(w_i)=\sqrt{\vert r_i-w_i\vert }
\]</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>【练习】给定了上一轮的预测结果<span class="math notranslate nohighlight">\(F_{m-1}(X_i)\)</span>和样本标签<span class="math notranslate nohighlight">\(y_i\)</span>，请计算使用平方损失时需要拟合的<span class="math notranslate nohighlight">\(w^*_i\)</span>。</p>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>【练习】当样本<span class="math notranslate nohighlight">\(i\)</span>计算得到的残差<span class="math notranslate nohighlight">\(r_i=0\)</span>时，本例中的函数在<span class="math notranslate nohighlight">\(w=0\)</span>处不可导，请问当前轮应当如何处理模型输出？</p>
</div>
<p>根据在零点处的梯度下降可知：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
w^*_i &amp;= 0 - \left.\frac{\partial L}{\partial w} \right|_{w=0}\\
&amp;= -\frac{1}{2\sqrt{r_i}}sign(r_i)
\end{aligned}
\end{split}\]</div>
<p>为了缓解模型的过拟合现象，我们需要引入学习率参数<span class="math notranslate nohighlight">\(\eta\)</span>来控制每轮的学习速度，即获得了由<span class="math notranslate nohighlight">\(\textbf{w}^*\)</span>拟合的第m棵树<span class="math notranslate nohighlight">\(h^*\)</span>后，当前轮的输出结果为</p>
<div class="math notranslate nohighlight">
\[
\hat{y}_i=F_{m-1}(X_i)+\eta h^*_m(X_i)
\]</div>
<p>对于上述的梯度下降过程，还可以从另一个等价的角度来观察：若设当前轮模型预测的输出值为<span class="math notranslate nohighlight">\(\tilde{w}_i= F_{m-1}(X_i)+w_i\)</span>，求解的问题即为</p>
<div class="math notranslate nohighlight">
\[
\tilde{w}_i=\arg\min_{\tilde{w}} L(y_i, \tilde{w})
\]</div>
<p>由于当<span class="math notranslate nohighlight">\(\tilde{w}=F_{m-1}(X_i)\)</span>时，损失函数的值就是上一轮预测结果的损失值，因此只需将<span class="math notranslate nohighlight">\(L\)</span>在<span class="math notranslate nohighlight">\(\tilde{w}\)</span>在<span class="math notranslate nohighlight">\(\tilde{w}=F_{m-1}(X_i)\)</span>的位置进行梯度下降，此时当前轮的预测值应为</p>
<div class="math notranslate nohighlight">
\[
\tilde{w}^*_i=F_{m-1}(X_i)-\left.\frac{\partial L}{\partial \tilde{w}} \right|_{\tilde{w}=F_{m-1}(X_i)}
\]</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>【练习】除了梯度下降法之外，还可以使用<a class="reference external" href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">牛顿法</a>来逼近最值点。请叙述基于牛顿法的GBDT回归算法。</p>
</div>
<p>从而当前轮学习器<span class="math notranslate nohighlight">\(h\)</span>需要拟合的目标值<span class="math notranslate nohighlight">\(w^*_i\)</span>为</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
w^*_i &amp;= \tilde{w}_i-F_{m-1}(X_i)\\
&amp;=0-\frac{\partial L}{\partial w} \left.\frac{\partial w}{\partial \tilde{w}} \right|_{\tilde{w}=F_{m-1}(X_i)} \\
&amp;= 0-\left.\frac{\partial L}{\partial w} \right|_{\tilde{w}=F_{m-1}(X_i)} \\
&amp;=  0 - \left.\frac{\partial L}{\partial w} \right|_{w=0}
\end{aligned}
\end{split}\]</div>
<p>上述的结果与先前的梯度下降结果完全一致，事实上这两种观点在本质上没有任何区别，只是损失函数本身进行了平移，下图展示了它们之间的联系。</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/gbdt_pic1.png"><img alt="../_images/gbdt_pic1.png" src="../_images/gbdt_pic1.png" style="width: 600px;" /></a>
</div>
<div class="admonition-gbdt admonition">
<p class="admonition-title">GBDT的特征重要性</p>
<p>在sklearn实现的GBDT中，特征重要性的计算方式与随机森林相同，即利用相对信息增益来度量单棵树上的各特征特征重要性，再通过对所有树产出的重要性得分进行简单平均来作为最终的特征重要性。</p>
</div>
</div>
<div class="section" id="id1">
<h2>2. 用于分类的GBDT<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h2>
<p>CART树能够同时处理分类问题和回归问题，但是对于多棵CART进行分类任务的集成时，我们并不能将树的预测结果直接进行类别加和。在GBDT中，我们仍然使用回归树来处理分类问题，那此时拟合的对象和流程又是什么呢？</p>
<p>对于<span class="math notranslate nohighlight">\(K\)</span>分类问题，我们假设得到了<span class="math notranslate nohighlight">\(K\)</span>个得分<span class="math notranslate nohighlight">\(F_{1i},...,F_{Ki}\)</span>来代表样本<span class="math notranslate nohighlight">\(i\)</span>属于对应类别的相对可能性，那么在进行Softmax归一化后，就能够得到该样本属于这些类别的概率大小。其中，属于类别k的概率即为<span class="math notranslate nohighlight">\(\frac{e^{F_{ki}}}{\sum_{c=1}^Ke^{F_{ci}}}\)</span>。此时，我们就能够使用多分类的交叉熵函数来计算模型损失，设<span class="math notranslate nohighlight">\(\textbf{y}_i=[y_{1i},...,y_{Ki}]\)</span>为第<span class="math notranslate nohighlight">\(i\)</span>个样本的类别独热编码，记<span class="math notranslate nohighlight">\(\textbf{F}_i=[F_{1i},...,F_{Ki}]\)</span>，则该样本的损失为</p>
<div class="math notranslate nohighlight">
\[
L(\textbf{y}_i,\textbf{F}_i)=-\sum_{c=1}^K y_{ci}\log \frac{e^{F_{ci}}}{\sum_{\tilde{c}=1}^Ke^{F_{\tilde{c}i}}}
\]</div>
<p>上述的<span class="math notranslate nohighlight">\(K\)</span>个得分可以由<span class="math notranslate nohighlight">\(K\)</span>棵回归树通过集成学习得到，树的生长目标正是使得上述的损失最小化。记第<span class="math notranslate nohighlight">\(m\)</span>轮中<span class="math notranslate nohighlight">\(K\)</span>棵树对第<span class="math notranslate nohighlight">\(i\)</span>个样本输出的得分为<span class="math notranslate nohighlight">\(\textbf{h}^{(m)}_i=[h^{(m)}_{1i},...,h^{(m)}_{Ki}]\)</span>，则此时<span class="math notranslate nohighlight">\(\textbf{F}^{(m)}_i=\textbf{F}^{(m-1)}_i+\textbf{h}^{(m)}_i\)</span>。与GBDT处理回归问题的思路同理，只需要令损失函数<span class="math notranslate nohighlight">\(L(\textbf{y}_i,\textbf{F}_i)\)</span>在<span class="math notranslate nohighlight">\(\textbf{F}_i=\textbf{F}_i^{(m-1)}\)</span>处梯度下降即可：</p>
<div class="math notranslate nohighlight">
\[
\textbf{F}_i^{*(m)} = \textbf{F}_i^{(m-1)} - \left.\frac{\partial L}{\partial \textbf{F}_i} \right|_{\textbf{F}_i=\textbf{F}_i^{(m-1)}}
\]</div>
<p>我们需要计算第二项中每一个梯度元素，即</p>
<div class="math notranslate nohighlight">
\[
-\left.\frac{\partial L}{\partial \textbf{F}_i} \right|_{\textbf{F}_i=\textbf{F}_i^{(m-1)}}=[-\left.\frac{\partial L}{\partial F_{1i}} \right|_{\textbf{F}_i=\textbf{F}_i^{(m-1)}},...,-\left.\frac{\partial L}{\partial F_{Ki}} \right|_{\textbf{F}_i=\textbf{F}_i^{(m-1)}}]
\]</div>
<p>对于第<span class="math notranslate nohighlight">\(k\)</span>个元素有</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
-\left.\frac{\partial L}{\partial F_{ki}} \right|_{\textbf{F}_i=\textbf{F}_i^{(m-1)}} &amp;= \left.\frac{\partial}{\partial F_{ki}} \sum_{c=1}^K y_{ci}\log \frac{e^{F_{ci}}}{\sum_{\tilde{c}=1}^Ke^{F_{\tilde{c}i}}} \right|_{\textbf{F}_i=\textbf{F}_i^{(m-1)}} \\
&amp;= \left.\frac{\partial}{\partial F_{ki}} \sum_{c=1}^K y_{ci} F_{ki} \right|_{\textbf{F}_i=\textbf{F}_i^{(m-1)}} -  \left.\frac{\partial}{\partial F_{ki}} \sum_{c=1}^K y_{ci} \log [\sum_{\tilde{c}=1}^K e^{F_{\tilde{c}i}}] \right|_{\textbf{F}_i=\textbf{F}_i^{(m-1)}} \\
&amp;= y_{ki} -  \left.\frac{\partial}{\partial F_{ki}} \sum_{c=1}^K y_{ci} \log [\sum_{\tilde{c}=1}^K e^{F_{\tilde{c}i}}] \right|_{\textbf{F}_i=\textbf{F}_i^{(m-1)}}
\end{aligned}
\end{split}\]</div>
<p>由于在上式的第二项里，<span class="math notranslate nohighlight">\(K\)</span>个<span class="math notranslate nohighlight">\(y_{ci}\)</span>中只有一个为<span class="math notranslate nohighlight">\(1\)</span>，且其余为<span class="math notranslate nohighlight">\(0\)</span>，从而得到</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
-\left.\frac{\partial L}{\partial F_{ki}} \right|_{\textbf{F}_i=\textbf{F}_i^{(m-1)}} &amp;= y_{ki} -  \left.\frac{\partial}{\partial F_{ki}} \log [\sum_{\tilde{c}=1}^K e^{F_{\tilde{c}i}}] \right|_{\textbf{F}_i=\textbf{F}_i^{(m-1)}} \\
&amp;= y_{ki} - \frac{e^{F^{(m-1)}_{ki}}}{\sum_{c=1}^K e^{F^{(m-1)}_{ci}}}
\end{aligned}
\end{split}\]</div>
<p>此时，<span class="math notranslate nohighlight">\(K\)</span>棵回归树的学习目标为：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\textbf{h}_i^{*(m)} &amp;= \textbf{F}_i^{*(m)} - \textbf{F}_i^{(m-1)}\\
&amp;= - \left.\frac{\partial L}{\partial \textbf{F}_i} \right|_{\textbf{F}_i=\textbf{F}_i^{(m-1)}} \\
&amp;= [y_{1i} - \frac{e^{F^{(m-1)}_{1i}}}{\sum_{c=1}^K e^{F^{(m-1)}_{ci}}},...,y_{Ki} - \frac{e^{F^{(m-1)}_{Ki}}}{\sum_{c=1}^K e^{F^{(m-1)}_{ci}}}]
\end{aligned}
\end{split}\]</div>
<p>同时，为了减缓模型的过拟合现象，模型在第<span class="math notranslate nohighlight">\(m\)</span>轮实际的<span class="math notranslate nohighlight">\(\textbf{F}^{*(m)}_i\)</span>为<span class="math notranslate nohighlight">\(\textbf{F}_i^{(m-1)}+\eta \textbf{h}_i^{*(m)}\)</span>。</p>
<p>由于每一轮都需要进行<span class="math notranslate nohighlight">\(K\)</span>棵树的拟合，因此GBDT在处理多分类时的速度较慢。事实上，我们可以利用概率和为<span class="math notranslate nohighlight">\(1\)</span>的性质，将<span class="math notranslate nohighlight">\(K\)</span>次拟合减少至<span class="math notranslate nohighlight">\(K-1\)</span>次拟合，这在处理类别数较少的分类问题时，特别是在处理二分类问题时，是非常有用的。</p>
<p>具体来说，此时我们需要<span class="math notranslate nohighlight">\(K-1\)</span>个得分，记为<span class="math notranslate nohighlight">\(F_{1i},...,F_{(K-1)i}\)</span>，则样本相应属于<span class="math notranslate nohighlight">\(K\)</span>个类别的概率值可表示为</p>
<div class="math notranslate nohighlight">
\[
[\frac{e^{F_{1i}}}{1+\sum_{c=1}^{K-1}e^{F_{ci}}},...,\frac{e^{F_{(K-1)i}}}{1+\sum_{c=1}^{K-1}e^{F_{ci}}},\frac{1}{1+\sum_{c=1}^{K-1}e^{F_{ci}}}]
\]</div>
<p>当<span class="math notranslate nohighlight">\(K\geq3\)</span>时，仍然使用独热编码来写出损失函数：</p>
<div class="math notranslate nohighlight">
\[
L(F_{1i},...,F_{(K-1)i})= y_{Ki}\log [1+\sum_{c=1}^{K-1}e^{F_{ci}}] -\sum_{c=1}^{K-1} y_{ci}\log \frac{e^{F_{ci}}}{\sum_{c=1}^Ke^{F_{ci}}} 
\]</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>【练习】请验证多分类负梯度的结果。</p>
</div>
<p>类似地记<span class="math notranslate nohighlight">\(\textbf{F}_i=[F_{1i},...,F_{(K-1)i}]\)</span>，我们可以求出负梯度：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
-\left.\frac{\partial L}{\partial F_{ki}} \right|_{\textbf{F}_i=\textbf{F}_i^{(m-1)}} = \left\{
\begin{aligned}
-\frac{e^{F^{(m-1)}_{ki}}}{\sum_{c=1}^{K-1} e^{F^{(m-1)}_{ci}}}  &amp;\qquad y_{Ki}=1 \\
y_{ki} - \frac{e^{F^{(m-1)}_{ki}}}{\sum_{c=1}^{K-1} e^{F^{(m-1)}_{ci}}} &amp; \qquad y_{Ki}=0 \\
\end{aligned}
\right.
\end{split}\]</div>
<p>当<span class="math notranslate nohighlight">\(K=2\)</span>时，不妨规定<span class="math notranslate nohighlight">\(y_i\in \{0,1\}\)</span>，此时损失函数可简化为</p>
<div class="math notranslate nohighlight">
\[
L(F_i) = - y_i\log \frac{e^{F_i}}{1+e^{F_i}} - (1-y_i)\log \frac{1}{1+e^{F_i}}
\]</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>【练习】请验证二分类负梯度的结果。</p>
</div>
<p>负梯度为</p>
<div class="math notranslate nohighlight">
\[
-\left.\frac{\partial L}{\partial F_{i}} \right|_{F_i=F^{(m-1)}_i}=y_i-\frac{e^{F_i}}{1+e^{F_i}} 
\]</div>
<p>最后，我们可以使用各个类别在数据中的占比情况来初始化<span class="math notranslate nohighlight">\(\textbf{F}^{(0)}\)</span>。具体地说，设各类别比例为<span class="math notranslate nohighlight">\(p_1,...,p_K\)</span>（<span class="math notranslate nohighlight">\(K\geq3\)</span>），我们希望初始模型的参数<span class="math notranslate nohighlight">\(F^{(0)}_1,...,F^{(0)}_{K-1}\)</span>满足</p>
<div class="math notranslate nohighlight">
\[
[\frac{e^{F^{(0)}_{1i}}}{1+\sum_{c=1}^{K-1}e^{F^{(0)}_{ci}}},...,\frac{e^{F^{(0)}_{(K-1)i}}}{1+\sum_{c=1}^{K-1}e^{F^{(0)}_{ci}}},\frac{1}{1+\sum_{c=1}^{K-1}e^{F^{(0)}_{ci}}}] = [p_1,...,p_{K-1},p_K]
\]</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>【练习】设二分类数据集中正样本比例为<span class="math notranslate nohighlight">\(10\%\)</span>，请计算模型的初始参数<span class="math notranslate nohighlight">\(F^{(0)}\)</span>。</p>
</div>
<p>对二分类（0-1分类）而言，设正负样本占比分别为<span class="math notranslate nohighlight">\(p_1\)</span>和<span class="math notranslate nohighlight">\(p_0\)</span>，则初始模型参数<span class="math notranslate nohighlight">\(F^{(0)}\)</span>应当满足</p>
<div class="math notranslate nohighlight">
\[
[ \frac{1}{1+e^{F^{(0)}_i}},\frac{e^{F^{(0)}_i}}{1+e^{F^{(0)}_i}}]=[p_0,p_1]
\]</div>
</div>
<div class="section" id="id2">
<h2>3. GBDT中的并行策略<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
</div>
<div class="section" id="id3">
<h2>代码实践<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h2>
</div>
<div class="section" id="id4">
<h2>算法实现<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h2>
</div>
<div class="section" id="id5">
<h2>知识回顾<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h2>
</div>
</div>


              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="03_ada.html" title="previous page">Part C: 自适应提升法</a>
    <a class='right-next' id="next-link" href="05_xgb_lgb.html" title="next page">Part E: 梯度提升树（下）</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By GYH<br/>
        
            &copy; Copyright 2021, GYH.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>